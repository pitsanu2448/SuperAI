#1. inatall and import Libraries
!pip install -q prophet
!pip install -q statsforecast
!pip install -q datasetsforecast

import datetime
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from time import time
from scipy import stats
from prophet import Prophet
from statsforecast import StatsForecast
from statsmodels.tsa.seasonal import STL
from sklearn.preprocessing import MinMaxScaler
from datasetsforecast.losses import mape
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.seasonal import seasonal_decompose
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import itertools
import warnings

plt.style.use('grayscale') # fivethirtyeight  grayscale  classic
plt.rcParams['lines.linewidth'] = 1.5
dark_style = {
    'figure.facecolor': '#008080',  # #212946
    'axes.facecolor': '#008080',
    'savefig.facecolor': '#008080',
    'axes.grid': True,
    'axes.grid.which': 'both',
    'axes.spines.left': False,
    'axes.spines.right': False,
    'axes.spines.top': False,
    'axes.spines.bottom': False,
    'grid.color': '#000000',  #2A3459
    'grid.linewidth': '1',
    'text.color': '0.9',
    'axes.labelcolor': '0.9',
    'xtick.color': '0.9',
    'ytick.color': '0.9',
    'font.size': 12 }
plt.rcParams.update(dark_style)

from pylab import rcParams
rcParams['figure.figsize'] = (18,7)

# Load the dataset
train_path = '/kaggle/input/forecasting-electricity-consumption/train.csv'
submission_path = '/kaggle/input/forecasting-electricity-consumption/sample_submission.csv'

df = pd.read_csv(train_path)
df_submission = pd.read_csv(submission_path)

# Create a DataFrame with the 'ds' column
df1 = pd.DataFrame({'ds': [None] * 114})

# Define the start and end dates
start_date = datetime.datetime(2014, 1, 1)
end_date = datetime.datetime(2023, 6, 1)

# Calculate the date range
date_range = pd.date_range(start_date, end_date, freq='MS')

# Append the formatted dates to the 'ds' column
df1['ds'] = date_range.strftime('%Y-%m')
df.insert(1,'ds', df1['ds'])
# Convert ds column to datetime
df['ds'] = pd.to_datetime(df['ds'], format='%Y-%m')
# Create a new column 'y' and calculate the sum of y1, y2, and y3
df['y'] = df[['y1', 'y2', 'y3']].sum(axis=1)
df.columns = ['unique_id', 'ds', 'y1', 'y2', 'y3', 'y']
df['unique_id'] = 'PEA_Load_MS'
df = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)
print(df.info())
df.head(1)

# Create an empty DataFrame with 12 rows
df_pre0 = pd.DataFrame({'id': [None] * 12})
# Create a list of integers from 1 to 12
int_list = list(range(1, 13))
# Append the integer list to the 'id' column
df_pre0['id'] = int_list

df3 = pd.DataFrame({'ds': [None] * 12})
# Define the start and end dates for each segment
start_date_1 = datetime.datetime(2023, 7, 1)
end_date_1 = datetime.datetime(2024, 6, 1)

# Calculate the date ranges for each segment
date_range_1 = pd.date_range(start_date_1, end_date_1, freq='MS')

# Append the formatted dates to the 'ds' column for each segment
df3['ds'].iloc[0:12] = date_range_1.strftime('%Y-%m')
#Assign df3 to df_results['ds']
df_pre0['ds']=df3
# Convert ds column to datetime
df_pre0['ds'] = pd.to_datetime(df_pre0['ds'], format='%Y-%m')
print(df_pre0.info())
df_pre0

#Stationarity check by ADF test
from statsmodels.tsa.stattools import adfuller
def Augmented_Dickey_Fuller_Test_func(series , column_name):
    print (f'Dickey-Fuller test results for columns: {column_name}')
    dftest = adfuller(series, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    print (dfoutput)
    if dftest[1] <= 0.05:
        print("Conclusion:====>")
        print("Reject the null hypothesis")
        print("The data is stationary")
    else:
        print("Conclusion:====>")
        print("The null hypothesis cannot be rejected")
        print("The data is not stationary")
        
Augmented_Dickey_Fuller_Test_func(df['y'],"Life expectancy")

#Autocorrelation and Partial Autocorrelation
ig, axs = plt.subplots(nrows=1, ncols=2)

plot_acf(df["y"],  lags=30, ax=axs[0],color="fuchsia")
axs[0].set_title("Autocorrelation");

# Grafico
plot_pacf(df["y"],  lags=30, ax=axs[1],color="lime")
axs[1].set_title('Partial Autocorrelation')

plt.show();

#Defind and plot Seasonal Decompose
def plotSeasonalDecompose(
    x,
    model='additive',
    filt=None,
    period=None,
    two_sided=True,
    extrapolate_trend=0,
    title="Seasonal Decomposition"):
    result = seasonal_decompose(
            x, model=model, filt=filt, period=period,
            two_sided=two_sided, extrapolate_trend=extrapolate_trend)
    fig = make_subplots(
            rows=4, cols=1,
            subplot_titles=["Observed", "Trend", "Seasonal", "Residuals"])
    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):
        fig.add_trace(
            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),
                row=idx+1, col=1,
            )
    return fig

plotSeasonalDecompose(
    df['y'],
    model="additive",
    period=12,
    title="Seasonal Decomposition")

external_df = pd.read_csv('/kaggle/input/pea-load/pea_load_df.csv')
external_df.tail(1)

external_df['ds'] = pd.to_datetime(external_df['ds'])
external_df.rename(columns={'y': 'Total'}, inplace=True)
df = external_df.copy()
# Extrect trend and seasonal by MSTL Decompose function
# Define seasonal periods (12)
seasonal_periods = [12]
def mstl_decompose(series, seasonal_periods):
    """
    Perform Multiple Seasonal-Trend decomposition using STL.
    Args:
        series (pd.Series): Time series data.
        seasonal_periods (list): List of seasonal periods to decompose.
    Returns:
        dict: Decomposed components including trend, each seasonal component, and residual.
    """
    residual = series.copy()
    components = {}

    for i, period in enumerate(seasonal_periods):
        stl = STL(residual, period=period, robust=True)
        result = stl.fit()
        seasonal = result.seasonal
        trend = result.trend
        resid = result.resid
        components[f'seasonal_{i+1}'] = seasonal
        residual = resid  # Update residual for next seasonality
    
    # The final residual after removing all seasonal components
    components['trend'] = trend
    components['resid'] = residual
    return components

# MSTL to all product series
mstl_components = {}
for col in external_df.columns:
    if col.startswith('Total'):
        mstl_components[col] = mstl_decompose(external_df[col], seasonal_periods)
        external_df['trend'] = mstl_components[col]['trend']
        external_df['seasonal'] = sum([mstl_components[col][f'seasonal_{i+1}'] for i in range(len(seasonal_periods))])
external_df.head(1)

#External factor select columns
external_df = external_df[['ds', 'Total', 'trend', 'seasonal']]
# Select relevant columns and perform normalization
for col in ['Total', 'trend', 'seasonal']:
    scaler = MinMaxScaler(feature_range=(0, 1))
    external_df[f"{col}_normalized"] = scaler.fit_transform(external_df[[col]])

# Merge external factors
df = pd.merge(df, external_df[['ds'] + [col for col in external_df if col.endswith('normalized')]], on='ds', how='left')

df.head(1)

# Update items_list
items_list = df.columns[1:-4].tolist()  # Exclude 'ds', 'Total', normalized external factors

# Split the data into train and validation sets
train_data = df[(df['ds'] < '2023-07-01')]
val_test_data = df[(df['ds'] >= '2023-07-01')]
print('val_test_data.shape', val_test_data.shape)
val_test_data.head(1)

# Step 5: Build the Prophet model for each product and forecast
from sklearn.metrics import mean_absolute_percentage_error
models = []
forecasts = []

# Fit Prophet models with external factors as regressors
for item in items_list:
    temp_df = train_data[['ds', item, 'Total_normalized', 'trend_normalized', 'seasonal_normalized']].rename(columns={item: 'y'})  # Prepare data for Prophet
    
    # Initialize the Prophet model and add external factors as regressors
    model = Prophet()
    for col in external_df.columns:  # Add all normalized external factors
        if col.endswith('normalized'):
            model.add_regressor(col)
    
    # Fit the model
    model.fit(temp_df)
    
    # Create future dataframe for forecast (july 2023 - June 2024)
    future = model.make_future_dataframe(periods=len(val_test_data), freq='MS')
    # Set the 'ds' column as the index for external_df
    #external_df.set_index('ds', inplace=True)    
    for col in external_df.columns: # Set future values for external factors
        if col.endswith('normalized'):
            # Filter future['ds'] to include only dates available in external_df index
            #valid_dates = future['ds'].isin(external_df.index)
        
            # Fill missing values in 'future' with forward-fill or some other strategy if necessary
            future[col] = external_df.set_index('ds').loc[future['ds'], col].values

            # Optionally, you can fill missing future data by forward-filling:
            #future[col].fillna(method='ffill', inplace=True)  # or use 'bfill'

    # Proceed with forecast or further processing
    forecast = model.predict(future)

    # Clip the forecast to ensure no negative values (since quantity must be >= 0)
    forecast['yhat'] = np.clip(forecast['yhat'], a_min=0, a_max=None)
    
    forecasts.append(forecast[['ds', 'yhat']])
    
    # Append model to list
    models.append(model)

# Combine all the forecasts into a single DataFrame
combined_forecasts = pd.concat([forecast.set_index('ds')['yhat'] for forecast in forecasts], axis=1)
combined_forecasts.columns = items_list

# Step 6: Evaluate the model using PAPE
forecast_val_test_period = combined_forecasts.loc[val_test_data['ds']]
mape_scores = {}

# Calculate mape for each product in validation period
for item in items_list:
    if item in val_test_data.columns:
        y_true = val_test_data[item]
        y_pred = forecast_val_test_period[item]
        mape = np.sqrt(mean_absolute_percentage_error(y_true, y_pred))
        mape_scores[item] = mape

# Calculate total mape
total_mape = np.mean(list(mape_scores.values()))
print(f"Total mape for the ProphetBestModel_item_reg_dest: {total_mape:.4f}")

combined_forecasts.to_csv('/kaggle/working/submit_pea_load_df1.csv', index=True)
