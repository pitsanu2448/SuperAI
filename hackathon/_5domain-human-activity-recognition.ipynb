{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q autogluon\n\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom autogluon.tabular import TabularPredictor\nfrom scipy.stats import iqr, skew, kurtosis  # Import the required functions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the path to the train and test folders\ntrain_path = \"/kaggle/input/super-ai-engineer-5-human-activity-recognition/HAR/train\"\ntest_path = \"/kaggle/input/super-ai-engineer-5-human-activity-recognition/HAR/test\"\n\n# Get a list of all CSV files in the train directory\ncsv_files = []\nfor activity_folder in sorted(os.listdir(train_path)):\n    activity_path = os.path.join(train_path, activity_folder)\n    for file in sorted(glob.glob(f\"{activity_path}/*.csv\")):\n        csv_files.append((file, activity_folder))  # Store file path and activity label\n\n# Read and concatenate all CSV files\ndf_list = []\nfor file, label in csv_files:\n    df = pd.read_csv(file)\n    df['Activity'] = label  # Add activity label to each file\n    df_list.append(df)\n\ndf = pd.concat(df_list, ignore_index=True)  # Combine into a single DataFrame\n\n# Display basic info\nprint(df.shape)\nprint(df.info())\ndf.head(2)\n\n# Load dataframe (df is assumed to have 6 columns: 6 features + 1 target)\nwindow_size = 500  # Window size for segmentation\n\n# Function to process data in segments\ndef process_segments(df, window_size):\n    num_segments = len(df) // window_size  # Number of windows\n    processed_data = []\n\n    for i in tqdm(range(num_segments), desc=\"Processing Segments\", unit=\"window\"):\n        start_idx = i * window_size\n        end_idx = (i + 1) * window_size\n        segment = df.iloc[start_idx:end_idx]  # Extract segment\n\n        # Dictionary to store aggregated features\n        feature_dict = {}\n\n        for col in df.columns[:-1]:  # Exclude the Activity column\n            data = segment[col].values\n\n            # Time-domain features\n            feature_dict[f'{col}_mean'] = np.mean(data)\n            feature_dict[f'{col}_std'] = np.std(data)\n            feature_dict[f'{col}_mad'] = np.median(np.abs(data - np.median(data)))\n            feature_dict[f'{col}_max'] = np.max(data)\n            feature_dict[f'{col}_min'] = np.min(data)\n            feature_dict[f'{col}_sma'] = np.sum(np.abs(data))  # Signal Magnitude Area\n            feature_dict[f'{col}_energy'] = np.sum(np.square(data))\n            feature_dict[f'{col}_iqr'] = iqr(data)  # Interquartile range\n            feature_dict[f'{col}_skewness'] = skew(data)  # Skewness\n            feature_dict[f'{col}_kurtosis'] = kurtosis(data)  # Kurtosis\n\n        # Assign the activity label (take the first row's label)\n        feature_dict['Activity'] = segment.iloc[0, -1]  # Keep first label of window\n\n        # Append the processed row\n        processed_data.append(feature_dict)\n\n    # Convert to a DataFrame\n    reduced_df = pd.DataFrame(processed_data)\n    return reduced_df\n\n# Apply the function\nreduced_df = process_segments(df, window_size)\n\n# Check the new shape\nprint(f\"Original shape: {df.shape}\")\nprint(f\"Reduced shape: {reduced_df.shape}\")\nprint(reduced_df.head())\n\n# Separate features (X) and target (y)\nX = reduced_df.drop(columns=['Activity'])  # Drop the target column\ny = reduced_df['Activity']  # Target variable\n\n# Initialize AutoGluon TabularPredictor\npredictor = TabularPredictor(label='Activity', eval_metric='accuracy').fit(train_data=reduced_df)\n\n# Load test data\ntest_files = sorted(glob.glob(f\"{test_path}/*.csv\"))\ntest_data = []\n\nfor file in test_files:\n    df = pd.read_csv(file)\n    df['ID'] = os.path.basename(file)  # Add ID column\n    test_data.append(df)\n\n# Concatenate all test data into a single DataFrame\ntest_df = pd.concat(test_data, ignore_index=True)\n\n# Apply feature engineering (reuse the process_segments function)\ntest_reduced_df = process_segments(test_df, window_size=500)\n\n# Check new test data shape\nprint(f\"Processed Test Data Shape: {test_reduced_df.shape}\")\nprint(test_reduced_df.head())\n\n# Split the ID column\nX_test = test_reduced_df.drop(columns=['Activity'])  # Remove Activity (it's actually ID)\nID_column = test_reduced_df['Activity'].rename('ID')  # Rename it to 'ID'\n\n# Ensure only features are used\nX_test_final = X_test  # No Activity since it's test data\n\n# Predict activity labels\ntest_predictions = predictor.predict(X_test_final)\n\n# Save results\ntest_reduced_df[\"Predicted_Activity\"] = test_predictions\ntest_reduced_df.to_csv(\"predicted_activities.csv\", index=False)\n\nprint(\"Predictions saved!\")\n\n# Rename columns for submission\ntest_reduced_df.rename(columns={'Activity': 'id'}, inplace=True)\ntest_reduced_df.rename(columns={'Predicted_Activity': 'class'}, inplace=True)\n\n# Prepare final submission DataFrame\nfinal_df = test_reduced_df[['id', 'class']]\n\n# Save to CSV\nfinal_df.to_csv(\"/kaggle/working/submission_har_autogluon.csv\", index=False)\n\n# Check if the file is saved\nprint(\"CSV file saved as submission_har_autogluon.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}