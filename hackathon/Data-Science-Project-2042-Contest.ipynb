{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9311812,"sourceType":"datasetVersion","datasetId":5639530},{"sourceId":9357239,"sourceType":"datasetVersion","datasetId":5672812}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary libraries\n!pip install pandas numpy prophet matplotlib scikit-learn\n!pip install statsforecast datasetsforecast","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Import the necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom prophet import Prophet\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import STL\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\nimport itertools\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom plotly.subplots import make_subplots\nfrom statsforecast import StatsForecast\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.style.use('grayscale') # fivethirtyeight  grayscale  classic\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor': '#008080',  # #212946\n    'axes.facecolor': '#008080',\n    'savefig.facecolor': '#008080',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#000000',  #2A3459\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12 }\nplt.rcParams.update(dark_style)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (18,7)\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Load the main dataset and preprocess\ndata_path = '/kaggle/input/cmu-dsproject2024/Data_withoutBillNo_clean.csv'\ndf_0 = pd.read_csv(data_path, low_memory=False)\nprint(df_0.info())\ndf_0.head(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2.1: Data preprocess for ITEM_CODE group\ndf = df_0.copy()\n# Replace '.' with '-'\ndf['ITEM_CODE'] = df['ITEM_CODE'].str.replace('.', '-')\n# Remove leading and trailing spaces\ndf['ITEM_CODE'] = df['ITEM_CODE'].str.strip()\n# DataFrame named 'df' with an 'ITEM_CODE' column\ndf['ITEM_CODE'] = df['ITEM_CODE'].astype(str)  # Convert to string\n# Replace values starting with '10203--01' with '10203-01'\ndf['ITEM_CODE'] = df['ITEM_CODE'].str.replace('10203--01', '10203-01')\ndf['ITEM_CODE'].head(1)\n\n# Convert 'BILL_DATE' to datetime and filter necessary columns for analysis\ndf['BILL_DATE'] = pd.to_datetime(df['BILL_DATE'], format='%Y%m%d')\n# Convert 'QTY' to strings and then to numeric, handling commas and non-numeric characters\ndf.loc[:, 'QTY'] = pd.to_numeric(df['QTY'].astype(str).str.replace(',', ''), errors='coerce')\n# Aggregate the data by BILL_DATE and ITEM_CODE, summing QTY and pivot each ITEM_CODE as a separate column\ndf = df.groupby(['BILL_DATE', 'ITEM_CODE'])['QTY'].sum().unstack(fill_value=0).reset_index() \n# Ensure that the 'BILL_DATE' is renamed to 'ds' (required by Prophet)\ndf.rename(columns={'BILL_DATE': 'ds'}, inplace=True)\n\n#Calculate the sum of columns 2 to end (item codes)\n# Select only numeric columns (assuming item codes are numeric)\nnumeric_cols = [col for col in df.columns if col not in ['ds', 'ITEM_CODE']]  # Exclude 'ds' and 'ITEM_CODE'\n# Calculate the sum of numeric columns\ndf['y'] = df[numeric_cols].sum(axis=1)\ndf['unique_id'] = 1\ndf.head(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2.2: Data preprocess for REG_Code group\ndf_reg = df_0.copy()\n# Remove leading and trailing spaces\ndf_reg['REG_Code'] = df_reg['REG_Code'].str.strip()\n# DataFrame named 'df' with an 'REG_Code' column\ndf_reg['REG_Code'] = df_reg['REG_Code'].astype(str)  # Convert to string\n# Convert 'BILL_DATE' to datetime and filter necessary columns for analysis\ndf_reg['BILL_DATE'] = pd.to_datetime(df_reg['BILL_DATE'], format='%Y%m%d')\n# Convert 'QTY' to strings and then to numeric, handling commas and non-numeric characters\ndf_reg.loc[:, 'QTY'] = pd.to_numeric(df_reg['QTY'].astype(str).str.replace(',', ''), errors='coerce')\n# Aggregate the data by BILL_DATE and REG_Code, summing QTY and pivot each REG_Code as a separate column\ndf_reg = df_reg.groupby(['BILL_DATE', 'REG_Code'])['QTY'].sum().unstack(fill_value=0).reset_index() \n# Ensure that the 'BILL_DATE' is renamed to 'ds' (required by Prophet)\ndf_reg.rename(columns={'BILL_DATE': 'ds'}, inplace=True)\ndf_reg.head(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2.3: Data preprocess for DEST_CODE group\ndf_dest = df_0.copy()\n# Remove leading and trailing spaces\ndf_dest['DEST_CODE'] = df_dest['DEST_CODE'].str.strip()\n# DataFrame named 'df' with an 'DEST_CODE' column\ndf_dest['DEST_CODE'] = df_dest['DEST_CODE'].astype(str)  # Convert to string\n# Convert 'BILL_DATE' to datetime and filter necessary columns for analysis\ndf_dest['BILL_DATE'] = pd.to_datetime(df_dest['BILL_DATE'], format='%Y%m%d')\n# Convert 'QTY' to strings and then to numeric, handling commas and non-numeric characters\ndf_dest.loc[:, 'QTY'] = pd.to_numeric(df_dest['QTY'].astype(str).str.replace(',', ''), errors='coerce')\n# Aggregate the data by BILL_DATE and DEST_CODE, summing QTY and pivot each DEST_CODE as a separate column\ndf_dest = df_dest.groupby(['BILL_DATE', 'DEST_CODE'])['QTY'].sum().unstack(fill_value=0).reset_index() \n# Ensure that the 'BILL_DATE' is renamed to 'ds' (required by Prophet)\ndf_dest.rename(columns={'BILL_DATE': 'ds'}, inplace=True)\ndf_dest.head(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: EDA for Stationarity check, Autocorrelation and Seasonal Decompose plot\n# Step 3.1: EDA of ITEM_Code group\n# Check the data type of 'y' column\nif df['y'].dtypes == 'object':\n    # Try converting the column to numeric (assuming it contains numbers as strings)\n    try:\n        df['y'] = pd.to_numeric(df['y'])\n    except:\n        # Handle the case where conversion fails (e.g., non-numeric strings)\n        print(\"Error: Could not convert 'y' column to numeric. Please clean the data.\")\n\nStatsForecast.plot(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3.2: Stationarity check by ADF test\ndef Augmented_Dickey_Fuller_Test_func(series , column_name):\n    print (f'Dickey-Fuller test results for columns: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of observations used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] <= 0.05:\n        print(\"Conclusion:====>\")\n        print(\"Reject the null hypothesis\")\n        print(\"The data is stationary\")\n    else:\n        print(\"Conclusion:====>\")\n        print(\"The null hypothesis cannot be rejected\")\n        print(\"The data is not stationary\")\n        \nAugmented_Dickey_Fuller_Test_func(df['y'],\"Life expectancy\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3.3: Plot Autocorrelation and Partial Autocorrelation\nfig, axs = plt.subplots(nrows=1, ncols=2)\nplot_acf(df['y'],  lags=30, ax=axs[0],color=\"fuchsia\")\naxs[0].set_title(\"Autocorrelation of Total Product\");\n# Grafico\nplot_pacf(df['y'],  lags=30, ax=axs[1],color=\"lime\")\naxs[1].set_title('Partial Autocorrelationof Total Product')\nplt.show();","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 3.4: Defind and plot Seasonal Decompose\ndef plotSeasonalDecompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n    title=\"Seasonal Decomposition\"):\n    result = seasonal_decompose(\n            x, model=model, filt=filt, period=period,\n            two_sided=two_sided, extrapolate_trend=extrapolate_trend)\n    fig = make_subplots(\n            rows=4, cols=1,\n            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"])\n    for idx, col in enumerate(['observed', 'trend', 'seasonal', 'resid']):\n        fig.add_trace(\n            go.Scatter(x=result.observed.index, y=getattr(result, col), mode='lines'),\n                row=idx+1, col=1,\n            )\n    return fig\n\nplotSeasonalDecompose(\n    df['y'],\n    model=\"additive\",\n    period=28,\n    title=\"Seasonal Decomposition\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Feature engineerin data handelig for feed model\n# Load the external factor dataset\nexternal_df = pd.read_csv('/kaggle/input/cmu-ds2024-ext-factor/Data_withoutBillNo_ext_fector.csv',  low_memory=False )\nexternal_df['ds'] = pd.to_datetime(external_df['ds'])\nexternal_df.rename(columns={'y': 'Total'}, inplace=True)\n\n# Extrect trend and seasonal by MSTL Decompose function\n# Define seasonal periods (28)\nseasonal_periods = [28]\ndef mstl_decompose(series, seasonal_periods):\n    \"\"\"\n    Perform Multiple Seasonal-Trend decomposition using STL.\n    Args:\n        series (pd.Series): Time series data.\n        seasonal_periods (list): List of seasonal periods to decompose.\n    Returns:\n        dict: Decomposed components including trend, each seasonal component, and residual.\n    \"\"\"\n    residual = series.copy()\n    components = {}\n\n    for i, period in enumerate(seasonal_periods):\n        stl = STL(residual, period=period, robust=True)\n        result = stl.fit()\n        seasonal = result.seasonal\n        trend = result.trend\n        resid = result.resid\n        components[f'seasonal_{i+1}'] = seasonal\n        residual = resid  # Update residual for next seasonality\n    \n    # The final residual after removing all seasonal components\n    components['trend'] = trend\n    components['resid'] = residual\n    return components\n\n# MSTL to all product series\nmstl_components = {}\nfor col in external_df.columns:\n    if col.startswith('Total'):\n        mstl_components[col] = mstl_decompose(external_df[col], seasonal_periods)\n        external_df['trend'] = mstl_components[col]['trend']\n        external_df['seasonal'] = sum([mstl_components[col][f'seasonal_{i+1}'] for i in range(len(seasonal_periods))])\nexternal_df.head(1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#External factor select columns\nexternal_df = external_df[['ds', 'gasohol_95', 'trend', 'seasonal']]\n# Select relevant columns and perform normalization\nfor col in ['gasohol_95', 'trend', 'seasonal']:\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    external_df[f\"{col}_normalized\"] = scaler.fit_transform(external_df[[col]])\n\ndf.rename(columns={'y': 'Total'}, inplace=True)\n# Drop the 'unique_id' column from df (if intended)\ndf = df.drop(columns=['unique_id'])\n# Merge df and df_reg on 'ds' column with left join\ndf = pd.merge(df, df_reg, on='ds', how='left')\n# Merge df and df_dest on 'ds' column with left join\ndf = pd.merge(df, df_dest, on='ds', how='left')\n# Merge external factors\ndf = pd.merge(df, external_df[['ds'] + [col for col in external_df if col.endswith('normalized')]], on='ds', how='left')\n\ndf.head(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_df = df.to_csv('/kaggle/working/input_df.csv', index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update items_list\nitems_list = df.columns[1:-4].tolist()  # Exclude 'ds', 'Total', normalized external factors\n\n# Split the data into train and validation sets\ntrain_data = df[(df['ds'] >= '2024-01-01') & (df['ds'] <= '2024-05-31')]\nval_test_data = df[(df['ds'] >= '2024-06-01') & (df['ds'] <= '2024-06-30')]\nprint('val_test_data.shape', val_test_data.shape)\nval_test_data.head(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Build the Prophet model for each product and forecast\nmodels = []\nforecasts = []\n\n# Fit Prophet models with external factors as regressors\nfor item in items_list:\n    temp_df = train_data[['ds', item, 'gasohol_95_normalized', 'trend_normalized', 'seasonal_normalized']].rename(columns={item: 'y'})  # Prepare data for Prophet\n    \n    # Initialize the Prophet model and add external factors as regressors\n    model = Prophet()\n    for col in external_df.columns:  # Add all normalized external factors\n        if col.endswith('normalized'):\n            model.add_regressor(col)\n    \n    # Fit the model\n    model.fit(temp_df)\n    \n    # Create future dataframe for forecast (June 2024)\n    future = model.make_future_dataframe(periods=len(val_test_data), freq='D')\n    for col in external_df.columns:  # Set future values for external factors\n        if col.endswith('normalized'):\n            future[col] = external_df.set_index('ds').loc[future['ds'], col].values\n    \n    # Forecast\n    forecast = model.predict(future)\n    \n    # Clip the forecast to ensure no negative values (since quantity must be >= 0)\n    forecast['yhat'] = np.clip(forecast['yhat'], a_min=0, a_max=None)\n    \n    forecasts.append(forecast[['ds', 'yhat']])\n    \n    # Append model to list\n    models.append(model)\n\n# Combine all the forecasts into a single DataFrame\ncombined_forecasts = pd.concat([forecast.set_index('ds')['yhat'] for forecast in forecasts], axis=1)\ncombined_forecasts.columns = items_list\n\n# Step 6: Evaluate the model using RMSE\nforecast_val_test_period = combined_forecasts.loc[val_test_data['ds']]\nrmse_scores = {}\n\n# Calculate RMSE for each product in validation period\nfor item in items_list:\n    if item in val_test_data.columns:\n        y_true = val_test_data[item]\n        y_pred = forecast_val_test_period[item]\n        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n        rmse_scores[item] = rmse\n\n# Calculate total RMSE\ntotal_rmse = np.mean(list(rmse_scores.values()))\nprint(f\"Total RMSE for the ProphetBestModel_item_reg_dest: {total_rmse:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Save the forecasted results\n# Save the forecast to a CSV file\ncombined_forecasts.to_csv('/kaggle/working/submit_cmu_ds_ProphetBestModel_item_reg_dest.csv', index=True)\nprint(\"Forecast saved to 'submit_cmu_ds_ProphetBestModel_item_reg_dest.csv'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 8: Visualization of Forecasts vs Actuals\nselected_series = ['00000-00', '99999-000', '99999-006', 'R30', 'D32']\n\n# Plot actual vs forecasted values for selected series\nfor series in selected_series:\n    if series not in train_data.columns:\n        print(f\"Column '{series}' not found in training data. Skipping.\")\n        continue\n    if series not in val_test_data.columns:\n        print(f\"Column '{series}' not found in validation data. Skipping.\")\n        continue\n\n    # Plot the data if the column exists in both datasets\n    plt.figure(figsize=(12, 6))\n    plt.plot(train_data['ds'], train_data[series], label='Training Data')\n    plt.plot(val_test_data['ds'], val_test_data[series], label='Actual')\n    plt.plot(combined_forecasts.index, combined_forecasts[series], label='Forecast')\n\n    plt.title(f\"Forecast QTY for CODE {series} by ProphetBestModel\")\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Analyze RMSE Distribution\nimport seaborn as sns\n\nrmse_df = pd.DataFrame.from_dict(rmse_scores, orient='index', columns=['RMSE'])\nplt.figure(figsize=(10, 6))\nsns.histplot(rmse_df['RMSE'], bins=50, kde=True)\nplt.title('Distribution of RMSE Scores Across QTY of Products by ProphetBestModel')\nplt.xlabel('RMSE')\nplt.ylabel('Frequency')\nplt.show()\n\n# Save RMSE scores to CSV\nrmse_df.to_csv('/kaggle/working/rmse_scores_ProphetBestModel_item_reg_dest.csv')\nprint(\"RMSE scores saved to 'rmse_scores_ProphetBestModel_item_reg_dest.csv'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}