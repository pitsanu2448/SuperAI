{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91438,"databundleVersionId":11038850,"sourceType":"competition"}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport cv2\nimport torch\nimport random\nimport scipy\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import fftpack as fft\nfrom PIL import Image, ImageOps\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom timm.scheduler.cosine_lr import CosineLRScheduler\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport torchvision.transforms as T\nfrom timm.models import maxxvit","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the paths\ntrain_npy_path = \"/kaggle/input/signal-fast-radio-burst-detection/train/train\"\ntrain_csv_path = \"/kaggle/input/signal-fast-radio-burst-detection/train-labels-corrected/train\"\ntest_npy_path = \"/kaggle/input/signal-fast-radio-burst-detection/test/test\"\noutput_dir = \"/kaggle/working\"\ntrain_dir = os.path.join(output_dir, \"train_dir\")\n#val_dir = os.path.join(output_dir, \"val_dir\")\ntest_dir = os.path.join(output_dir, \"test_dir\")\n\n# Create output directories\nos.makedirs(train_dir, exist_ok=True)\n#os.makedirs(val_dir, exist_ok=True)\nos.makedirs(test_dir, exist_ok=True)\n\n# Load CSV files\ncsv_files = [f for f in os.listdir(train_csv_path) if f.endswith('.csv')]\ntrain_df = pd.DataFrame(columns=['index', 'image', 'label'])\n\n# Process each .npy file and corresponding .csv file\nfor csv_file in csv_files:\n    npy_file = csv_file.replace('_labels.csv', '.npy')\n    npy_data = np.load(os.path.join(train_npy_path, npy_file))  # Shape: (262144, 256)\n    csv_data = pd.read_csv(os.path.join(train_csv_path, csv_file))\n    \n    # Slice the npy_data into chunks of 256x256\n    num_chunks = npy_data.shape[0] // 256  # 262144 / 256 = 1024 chunks\n    for chunk_idx in range(num_chunks):\n        # Extract a 256x256 chunk\n        chunk = npy_data[chunk_idx * 256 : (chunk_idx + 1) * 256, :]  # Shape: (256, 256)\n        \n        # Save the chunk as an image\n        image_path = os.path.join(output_dir, f\"{npy_file.replace('+', '_').replace('-', '_').replace('.npy', '')}_{chunk_idx}.jpg\")\n        plt.imsave(image_path, chunk, cmap='gray')\n        \n        # Get the corresponding label from the CSV file\n        label = csv_data.iloc[chunk_idx]['labels']\n        \n        # Append the new row to the DataFrame using pd.concat\n        new_row = pd.DataFrame([{'index': f\"{npy_file.replace('+', '_').replace('-', '_').replace('.npy', '')}_{chunk_idx}\", 'image': image_path, 'label': label}])\n        train_df = pd.concat([train_df, new_row], ignore_index=True)\n\ntrain_df.to_csv('/kaggle/working/train_input_df.csv', index=False)\n# Display the first few rows of the DataFrame\nprint(train_df.head(2))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[\"label\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Alternative, slightly more concise approach (but less flexible):\nplt.figure(figsize=(10, 5)) # Create the figure *before* the hist call\ntrain_df['label'].hist()\nplt.title('Distribution of Labels')\nplt.xlabel('Label Value')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def drop_unknown_labels(df):\n    labels_to_drop = [\"Uncertain\", \"Unlabeled\"]\n    # Use isin() for efficient filtering\n    mask = train_df['label'].isin(labels_to_drop)\n    train_df.drop(train_df[mask].index, inplace=True)   \ndrop_unknown_labels(train_df)\n\n# Replace train_df['label']with 'None'\ntrain_df.fillna(0, inplace=True)\ntrain_df.replace(0, \"None\", inplace=True)  # Correct!\n\ntrain_df = pd.concat([\n  train_df[train_df['label']==\"None\"].sample(n=1500,replace=True),\n  train_df[train_df['label']==\"Pulse\"].sample(n=1500,replace=True),\n  train_df[train_df['label']==\"Broad\"].sample(n=1500,replace=True),\n  train_df[train_df['label']==\"Narrow\"].sample(n=1500,replace=True),\n  train_df[train_df['label']==\"Broad+Pulse\"].sample(n=1500,replace=True),\n  train_df[train_df['label']==\"Narrow+Broad\"].sample(n=1500,replace=True),\n  train_df[train_df['label']==\"Narrow+Pulse\"].sample(n=1500,replace=True)\n  #train_df[train_df['label']==\"Unknown+Pulse\"].sample(n=1500,replace=True)\n  #train_df[train_df['label']==\"8\"].sample(n=1500,replace=True),\n  #train_df[train_df['label']==\"9\"].sample(n=1500,replace=True)\n  ],axis=0)\n\nplt.figure(figsize=(10, 5)) # Create the figure *before* the hist call\ntrain_df['label'].hist()\nplt.title('Distribution of Labels')\nplt.xlabel('Label Value')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a mapping from string labels to integer indices\nlabel_mapping = {\n    \"None\": 0,\n    \"Pulse\": 1,\n    \"Broad\": 2,\n    \"Narrow\": 3,\n    \"Broad+Pulse\": 4,\n    \"Narrow+Broad\": 5,\n    \"Narrow+Pulse\": 6,\n    #\"Unknown+Pulse\" : 7,\n    #\"Uncertain\": 8,\n    #\"Unlabeled\" : 9\n}\ntrain_df['label'] = train_df['label'].map(label_mapping)\n\n# Split the dataset into training and validation sets\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the ImageDataset class\nclass ImageDataset(Dataset):\n    def __init__(self, dataframe, transforms, has_labels=True):\n        self.dataframe = dataframe\n        self.transforms = transforms\n        self.has_labels = has_labels  # Flag to indicate whether labels are present\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image = Image.open(row['image']).convert(\"RGB\")\n        image = self.transforms(image)\n        \n        if self.has_labels:\n            label = torch.tensor(row['label'], dtype=torch.long)  # Ensure label is of type torch.long\n            return image, label\n        else:\n            return image  # Return only the image if labels are not present\n\n# Define the transforms\nimg_size = (256, 256)\ntransforms = {\n    \"train\": T.Compose([\n        T.Resize(img_size, interpolation=T.InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=torch.tensor([0.5, 0.5, 0.5]), std=torch.tensor([0.23, 0.23, 0.23]))\n    ]),\n    \"test\": T.Compose([\n        T.Resize(img_size, interpolation=T.InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=torch.tensor([0.5, 0.5, 0.5]), std=torch.tensor([0.23, 0.23, 0.23]))\n    ]),\n}\n\ntrain_dataset = ImageDataset(train_df, transforms[\"train\"], has_labels=True)\nval_dataset = ImageDataset(val_df, transforms[\"test\"], has_labels=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = maxxvit.maxxvitv2_nano_rw_256(pretrained=True, num_classes=len(label_mapping)).to(device)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n\n# Training loop\nn_epochs = 4\nfor epoch in range(n_epochs):\n    model.train()\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n    \n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            val_loss += criterion(outputs, labels).item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss/len(val_loader)}, Val Acc: {correct/total}')\n\n# Save the model\n#torch.save(model.state_dict(), \"maxxvitv2_nano_rw_256.pth\")\n\n# Process the test set\ntest_files = [f for f in os.listdir(test_npy_path) if f.endswith('.npy')]\ntest_df = pd.DataFrame(columns=['id', 'image'])\n\nfor test_file in test_files:\n    npy_data = np.load(os.path.join(test_npy_path, test_file))  # Shape: (262144, 256)\n    num_chunks = npy_data.shape[0] // 256  # 262144 / 256 = 1024 chunks\n    for chunk_idx in range(num_chunks):\n        # Extract a 256x256 chunk\n        chunk = npy_data[chunk_idx * 256 : (chunk_idx + 1) * 256, :]  # Shape: (256, 256)\n        \n        # Save the chunk as an image\n        image_path = os.path.join(test_dir, f\"{test_file.replace('.npy', '')}_{chunk_idx}.jpg\")\n        plt.imsave(image_path, chunk, cmap='gray')\n        \n        # Append the new row to the DataFrame using pd.concat\n        new_row = pd.DataFrame([{'id': f\"{test_file.replace('.npy', '')}_{chunk_idx}\", 'image': image_path}])\n        test_df = pd.concat([test_df, new_row], ignore_index=True)\n\n# Create test dataset and dataloader\n#test_dataset = ImageDataset(test_df, transforms[\"test\"])\ntest_dataset = ImageDataset(test_df, transforms[\"test\"], has_labels=False)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n\n# Make predictions on the test set\nmodel.eval()\npredictions = []\nwith torch.no_grad():\n    for images in test_loader:  # No labels returned\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = outputs.max(1)\n        predictions.extend(predicted.cpu().numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame(columns=['id', 'pulse', 'broad', 'narrow'])\nfor idx, row in test_df.iterrows():\n    pred = predictions[idx]\n    if pred == 0:  # None\n        submission = pd.concat([submission, pd.DataFrame([{'id': row['id'], 'pulse': 0, 'broad': 0, 'narrow': 0}])], ignore_index=True)\n    elif pred == 1 :  # Pulse \n        submission = pd.concat([submission, pd.DataFrame([{'id': row['id'], 'pulse': 1, 'broad': 0, 'narrow': 0}])], ignore_index=True)\n    elif pred == 2:  # Broad\n        submission = pd.concat([submission, pd.DataFrame([{'id': row['id'], 'pulse': 0, 'broad': 1, 'narrow': 0}])], ignore_index=True)\n    elif pred == 3:  # Narrow\n        submission = pd.concat([submission, pd.DataFrame([{'id': row['id'], 'pulse': 0, 'broad': 0, 'narrow': 1}])], ignore_index=True)\n    elif pred == 4:  # Broad+Pulse or Pulse+Broad\n        submission = pd.concat([submission, pd.DataFrame([{'id': row['id'], 'pulse': 1, 'broad': 1, 'narrow': 0}])], ignore_index=True)\n    elif pred == 5:  # Narrow+Broad orBroad+Narrow\n        submission = pd.concat([submission, pd.DataFrame([{'id': row['id'], 'pulse': 0, 'broad': 1, 'narrow': 1}])], ignore_index=True)\n    elif pred == 6:  # Narrow+Pulse or Pulse+Narrow\n        submission = pd.concat([submission, pd.DataFrame([{'id': row['id'], 'pulse': 1, 'broad': 0, 'narrow': 1}])], ignore_index=True)\n\ndf0 = pd.read_csv('/kaggle/input/signal-fast-radio-burst-detection/sample_submission.csv')\ndf1 = df0['id']\nsubmission = pd.merge(df1, submission, on='id', how='left')\n# Save submission file\nsubmission.to_csv(os.path.join(output_dir, \"submission_maxxvitv2_nano_rw_256.sw_in1k.csv\"), index=False)\nprint(\"Copmplate to submission_maxxvitv2_nano_rw_256.sw_in1k_1.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}